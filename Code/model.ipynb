{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Lunar Lander Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Reinforcement Learning (RL) is a branch of machine learning where an agent learns to make decisions by interacting with its environment. Instead of being told explicitly what to do, the agent discovers the best actions through trial and error, guided by rewards or penalties. A great example to showcase RL is teaching a robot to navigate a maze, learn from its mistakes, and eventually masters the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander\n",
    "The below Lunar Lander Demo illustrates the power of RL by teaching a spacecraft to succesfully land in the moon within the designated area.\n",
    "The spacecraft (the RL Agent) has four discrete actions available:\n",
    "- Power the main engine \n",
    "- Power the left engine\n",
    "- Power the right engine\n",
    "- Power no engine\n",
    "\n",
    "In addition, fuel is infinite for the agent to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the required libraries\n",
    "The lunar lander RL demo leverages the ´stable-baselines´. This library provides a set of reliable implementations of reinforcement learning algorithms in PyTorch. ´stable-baselines´ works together with ´gymnasium´ to create and visualise training environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary requirements\n",
    "The algorithm used for the lunar lander model is DeepQ Network.\n",
    "\n",
    "The Deep Q-Network (DQN) algorithm is a reinforcement learning method that combines Q-learning with deep neural networks. It enables an agent to learn optimal actions in high-dimensional, complex environments by approximating the Q-value function with a neural network. DQN uses experience replay to store and sample past experiences, breaking the correlation between consecutive updates, and employs a target network to stabilize learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Deep Q-Network\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "# Import evaluate_policy to enable evaluation of the agent's performance\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "- Algorithm: Deep-Q Network\n",
    "- Decisioning Policy: Multi-Layer Perception Policy (suitable for environments with vector-based observations, e.g., numerical state arrays)\n",
    "- Final value of $\\epsilon$ is set to 0.1 (the agent exploits its learned policy by taking the action with the highest Q-value with probability 1 - $\\epsilon$)\n",
    "- Target Q-Network update (with the weights of the policy Q-Network) is set to 250 training steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'LunarLander-v3'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    \"LunarLander-v3\",\n",
    "    exploration_final_eps=0.1,\n",
    "    target_update_interval=250,\n",
    "    verbose = 1 # prints model details on execution \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the untrained model\n",
    "To appreciate the power of RL we evaluate the reward of the inital model without training, which is expected to be poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean reward achieved by the agent is = -553.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isabe\\Desktop\\Reinforcement Learning\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create lunar lander environment for evaluation\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# Evaluate the agent's performance before training\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "print(f\"The mean reward achieved by the agent is = {mean_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save the agent\n",
    "We train the agent for a total of 10,000,000 steps and save the output for future loading in the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.learn(total_timesteps=int(1e7))\n",
    "\n",
    "# Save the agent\n",
    "model.save(\"dqn_lunar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model\n",
    "Once the agent has trained, its performance is re-evaluated and expected to increase significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean reward achieved by the agent is = 176.65\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent's performance after training\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True)\n",
    "\n",
    "print(f\"The mean reward achieved by the agent is = {mean_reward:.2f}\")\n",
    "\n",
    "del model # deletes the variable model to avoid interference with loading in demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
